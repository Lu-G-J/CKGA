{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example of adapting CKGA for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import argparse\n",
    "\n",
    "from adapter_models import CONTROLER, ADAPTER, MHA\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, BertConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the dataset has three sentences where the aspects are surrounded by [SEP]\n",
    "\n",
    "The entity_idx represents the entity number of the aspect in sub-DBpedia.\n",
    "\n",
    "The labels is the sentiment labels of the dataset, which are tri-categorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_idx :  tensor([118, 404, 657]) \n",
      " labels :  tensor([0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datasets = [\"Easy to [SEP] carry [SEP] , can be taken anywhere, can be hooked up to printers,headsets.\",\n",
    "            \"Very good [SEP] quality [SEP] and well made.\",\n",
    "            \"They are by far the easiest [SEP] systems [SEP] to actually learn about computers with.\"]\n",
    "entity_idx = torch.randint(0, 1000, (len(datasets),))\n",
    "labels = torch.randint(0, 3, (len(datasets),))\n",
    "print('entity_idx : ',entity_idx, '\\n', 'labels : ', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CKGA requires additional hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(jupyter=False):\n",
    "    def str2bool(v):\n",
    "        if isinstance(v, bool):\n",
    "            return v\n",
    "        if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "            return True\n",
    "        elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "            return False\n",
    "        else:\n",
    "            raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "    ###################################################################\n",
    "    # parameters of original models\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--bert_model', type=str, default='bert-base-uncased')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-3)\n",
    "    ###################################################################\n",
    "    # parameters of adapters\n",
    "    parser.add_argument('--adapter_gcn_hid_dim', type=int, default=300)\n",
    "    parser.add_argument('--adapter_score', type=int, default=0, choices=[i for i in range(-10,12,2)])\n",
    "    parser.add_argument('--adapter_gcn_out_dim', type=int, default=768)\n",
    "    parser.add_argument('--adapter_dropout', type=float, default=0.5)\n",
    "    parser.add_argument('--adapter_layer_num', type=int, default=2)\n",
    "    parser.add_argument('--adapter_freeze_emb', type=str2bool, default=True)\n",
    "    parser.add_argument('--adapter_mode', type=str, default='adapter',choices=['adapter', 'origin'])\n",
    "    parser.add_argument('--adapter_kge', type=str, default='transh',choices=['transe', 'transh','transr','rotate'])\n",
    "    parser.add_argument('--adapter_norm', type=str2bool, default='False')\n",
    "    \n",
    "    parser.add_argument('--fuse_mode', type=str, default='p', choices=['p','c'], help='plus or concatenate')\n",
    "    parser.add_argument('--train_model', type=str, default='d', choices=['j','d'], help='joint or dependent')\n",
    "    parser.add_argument('--origin_model_path', type=str, default='./best_origin_state_dict/laptop_acc_0.7665_f1_0.7202.pkl')\n",
    "    parser.add_argument('--origin_model_lr', type=float, default=1e-3)\n",
    "    \n",
    "    ###################################################################\n",
    "    if jupyter is True:\n",
    "        return parser.parse_args(args=[])\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a BERT model as original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORIGINAL_MODEL(nn.Module):\n",
    "    def __init__(self, args) -> None:\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(args.bert_model)\n",
    "        self.dense = nn.Linear(768, 3)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.bert(inputs['input_ids'])[1]\n",
    "        logits = self.dense(x)\n",
    "        # in this example, the aspect_emb is \n",
    "        return {'emb': x, 'classification':logits, 'aspect_emb':x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "args = get_args(jupyter=True)\n",
    "args.device = 'cpu'\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "original_model = ORIGINAL_MODEL(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the normal calculation, the output of Bert looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb  :  torch.Size([1, 768])\n",
      "classification  :  torch.Size([1, 3])\n",
      "aspect_emb  :  torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode_plus(datasets[0], return_tensors='pt')\n",
    "'''\n",
    "outputs:\n",
    "emb:    torch.Size([1, 768])\n",
    "classification: torch.Size([1, 3])\n",
    "aspect_emb: torch.Size([1, 768])\n",
    "'''\n",
    "outputs = original_model(inputs)\n",
    "for key in outputs:\n",
    "    print(key, ' : ', outputs[key].size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CKGA with path of the dataset.\n",
    "### User CONTROLER to package the original model (bert) and CKGA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = ADAPTER(f'./graph/laptop', args)\n",
    "mha = MHA(emb1_dim=768, emb2_dim=args.adapter_gcn_out_dim, hdim=args.adapter_gcn_out_dim, n_head=2)\n",
    "if args.fuse_mode == 'p':\n",
    "    controler = CONTROLER(original_model, adapter, mha, 768, 3)\n",
    "elif args.fuse_mode =='c':\n",
    "    controler = CONTROLER(original_model, adapter, mha, 768+args.adapter_gcn_out_dim, 3)\n",
    "model = controler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After adding CKGA, the operation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1960, -0.2256,  0.2316]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22990/597451045.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adapter_inputs = torch.tensor(entity_idx[0])\n"
     ]
    }
   ],
   "source": [
    "adapter(None, gcn=True)\n",
    "original_model_inputs = inputs\n",
    "adapter_inputs = torch.tensor(entity_idx[0])\n",
    "outputs = model(original_model_inputs, \n",
    "                adapter_inputs, \n",
    "                model=args.adapter_mode, \n",
    "                mode=args.fuse_mode)\n",
    "print(outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since a sample uses only a little entities from sub-DBpedia, we do not want to update the parameters of CKGA frequently. For this reason, we set two optimizers so that the parameters of CKGA and the original model can be updated separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = [\n",
    "    {\"params\": [p for p in model.origin_model.parameters() if p.requires_grad], \"lr\":args.origin_model_lr},\n",
    "    {\"params\": [p for p in model.classification.parameters() if p.requires_grad], \"lr\":args.learning_rate}, \n",
    "    {\"params\": [p for p in model.mha.parameters() if p.requires_grad], \"lr\":args.learning_rate}, \n",
    "]\n",
    "params2 = [\n",
    "    {\"params\": [p for p in model.adapter.parameters() if p.requires_grad], \"lr\":args.learning_rate}\n",
    "]\n",
    "optimizer1 = torch.optim.Adam(params1, lr=args.learning_rate, weight_decay=0)\n",
    "optimizer2 = torch.optim.Adam(params2, lr=args.learning_rate, weight_decay=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When training the model, the computational steps for each epoch are as follows:\n",
    "1. Let CKGA do one graph convolution operation.\n",
    "2. For each batch, instead of computing the graph convolution again, we directly take the result after the graph convolution operation, which can significantly reduce the frequency of graph convolutions operation.\n",
    "3. After each batch is computed, update the parameters of the original model\n",
    "4. After each epoch, update the parameters of CKGA, which can reduce the frequency of parameter update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26105/2006620719.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  adapter_inputs = torch.tensor(entity_idx[i])\n"
     ]
    }
   ],
   "source": [
    "# epoch\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for _ in range(2):\n",
    "    optimizer2.zero_grad()\n",
    "    model.adapter(None, gcn=True)\n",
    "    for i in range(len(datasets)):\n",
    "        optimizer1.zero_grad()\n",
    "        original_model_inputs = tokenizer.encode_plus(datasets[i], return_tensors='pt')\n",
    "        adapter_inputs = torch.tensor(entity_idx[i])\n",
    "        targets = torch.tensor([labels[i]])\n",
    "        outputs = model(original_model_inputs, \n",
    "                        adapter_inputs, \n",
    "                        model=args.adapter_mode, \n",
    "                        mode=args.fuse_mode)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer1.step()\n",
    "    optimizer2.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d931657f8858ac28e9aaed952a72b6ee33c80e6eba26d255eafd4a93893c915"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
